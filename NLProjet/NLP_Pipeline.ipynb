{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a6725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30316a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data Loading and Exploration\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "df = pd.read_csv(\"Stress.csv\")\n",
    "\n",
    "not_used_cols = ['subreddit','post_id','sentence_range','confidence','social_timestamp']\n",
    "df1 = df.drop(not_used_cols,axis=1)\n",
    "df1['label'].unique()\n",
    "#Text processing\n",
    "#importing required libraries (nltk,spacy,urllib,re etc.)\n",
    "#downloading required database (omw), wordnet, punkt, stopwords\n",
    "#cleaning data (with regualr expression)\n",
    "import nltk\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from spacy import load\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('omw-1.4') # Open Multilingual Wordnet, this is an lexical database \n",
    "nltk.download('wordnet') \n",
    "nltk.download('wordnet2022')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57391e",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90838a73",
   "metadata": {},
   "source": [
    "I created the textPocess function to clean text entirely. It removes brackets, URLs, escape characters, HTML tags, and non-alphanumeric characters, converts text to lowercase, tokenizes it, removes stopwords (with a minor issue), and lemmatizes words. It also includes basic error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = list(stopwords.words('english'))\n",
    "def textProcess(sent):\n",
    "    try:\n",
    "        # Replace square brackets, parentheses with spaces\n",
    "        sent = re.sub('[][)(]', ' ', sent)\n",
    "\n",
    "        # Remove URLs by checking if a word has a URL scheme\n",
    "        sent = [word for word in sent.split() if not urlparse(word).scheme]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        # Remove escape characters starting with '@'\n",
    "        sent = re.sub(r'\\@\\w+', '', sent)\n",
    "\n",
    "        # Remove HTML tags using regular expressions\n",
    "        sent = re.sub(re.compile(\"<.*?>\"), '', sent)\n",
    "\n",
    "        # Keep only letters and numbers, replace others with spaces\n",
    "        sent = re.sub(\"[^A-Za-z0-9]\", ' ', sent)\n",
    "\n",
    "        # Convert all words to lowercase\n",
    "        sent = sent.lower()\n",
    "\n",
    "        # Strip extra spaces from words and sentences\n",
    "        sent = [word.strip() for word in sent.split()]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # Remove stopwords (common words that don't carry much meaning)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lemmatize the words (convert words to their base form)\n",
    "        sent = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        return sent\n",
    "\n",
    "    except Exception as ex:\n",
    "        # Handle exceptions and print an error message\n",
    "        print(sent, \"\\n\")\n",
    "        print(\"Error \", ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c007289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['processed_text'] = df1['text'].apply(lambda text: textProcess(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
