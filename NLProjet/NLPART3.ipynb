{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174623c7",
   "metadata": {
    "id": "174623c7"
   },
   "source": [
    "# Notebook 3 : Deep learning with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CbXcFSfb6kHx",
   "metadata": {
    "id": "CbXcFSfb6kHx"
   },
   "source": [
    "In this notebook we want to explore deep learning techniques if this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8e9d9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af8e9d9a",
    "outputId": "884a3f33-a5c8-4fad-fed5-2541cb3d6d3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Data Loading and Exploration\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "df = pd.read_csv(\"Stress.csv\")\n",
    "\n",
    "not_used_cols = ['subreddit','post_id','sentence_range','confidence','social_timestamp']\n",
    "df1 = df.drop(not_used_cols,axis=1)\n",
    "df1['label'].unique()\n",
    "#Text processing\n",
    "#importing required libraries (nltk,spacy,urllib,re etc.)\n",
    "#downloading required database (omw), wordnet, punkt, stopwords\n",
    "#cleaning data (with regualr expression)\n",
    "import nltk\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "#from spacy import load\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('omw-1.4') # Open Multilingual Wordnet, this is an lexical database\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "_09PFosT1UeJ",
   "metadata": {
    "id": "_09PFosT1UeJ"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = list(stopwords.words('english'))\n",
    "def textProcess(sent):\n",
    "    try:\n",
    "        # Replace square brackets, parentheses with spaces\n",
    "        sent = re.sub('[][)(]', ' ', sent)\n",
    "\n",
    "        # Remove URLs by checking if a word has a URL scheme\n",
    "        sent = [word for word in sent.split() if not urlparse(word).scheme]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        # Remove escape characters starting with '@'\n",
    "        sent = re.sub(r'\\@\\w+', '', sent)\n",
    "\n",
    "        # Remove HTML tags using regular expressions\n",
    "        sent = re.sub(re.compile(\"<.*?>\"), '', sent)\n",
    "\n",
    "        # Keep only letters and numbers, replace others with spaces\n",
    "        sent = re.sub(\"[^A-Za-z0-9]\", ' ', sent)\n",
    "\n",
    "        # Convert all words to lowercase\n",
    "        sent = sent.lower()\n",
    "\n",
    "        # Strip extra spaces from words and sentences\n",
    "        sent = [word.strip() for word in sent.split()]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        tokens = word_tokenize(sent)\n",
    "\n",
    "        # Remove stopwords (common words that don't carry much meaning)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lemmatize the words (convert words to their base form)\n",
    "        sent = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        return sent\n",
    "\n",
    "    except Exception as ex:\n",
    "        # Handle exceptions and print an error message\n",
    "        print(sent, \"\\n\")\n",
    "        print(\"Error \", ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f3d1d6",
   "metadata": {
    "id": "86f3d1d6"
   },
   "outputs": [],
   "source": [
    "df1['processed_text'] = df1['text'].apply(lambda text: textProcess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52ec6c4",
   "metadata": {
    "id": "a52ec6c4"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24456/1406645049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\saved_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_SavedModelBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSavedModelBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0m_np_bfloat16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0m_np_float8_e4m3fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat8_e4m3fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0m_np_float8_e5m2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_ml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat8_e5m2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hLIDE4KO41z6",
   "metadata": {
    "id": "hLIDE4KO41z6"
   },
   "source": [
    "**The goal now is to find the best deep learning techniques to determind stress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5f2aa",
   "metadata": {
    "id": "84d5f2aa"
   },
   "source": [
    "Step 2: Train-Test Split and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XOAvvm0q4fAm",
   "metadata": {
    "id": "XOAvvm0q4fAm"
   },
   "source": [
    "  The dataset is tokenized and padded for input into the LSTM model and split into training and testing sets.\n",
    "  The model architecture is defined with an embedding layer, LSTM layer, and a dense output layer to visualize best option to have best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2299de3",
   "metadata": {
    "id": "a2299de3"
   },
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df1['processed_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df1['processed_text'])\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df1['label'], test_size=0.2, stratify=df1['label'], random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZmSqManv-Vby",
   "metadata": {
    "id": "ZmSqManv-Vby"
   },
   "source": [
    "Tokenization serves as a crucial step in transforming raw text into a format suitable for deep learning models.\n",
    "The creation of numerical representations essential for training models like LSTM.\n",
    "Now the model can understand and learn patterns within stress-related text\n",
    "Lets continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4530d",
   "metadata": {
    "id": "47c4530d"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50  # Adjust based on your choice\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=padded_sequences.shape[1]))\n",
    "model.add(LSTM(units=50, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))  # Assuming 3 stress levels\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0804a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c0804a1",
    "outputId": "d28d8b2f-0f30-4b6e-a1bb-b60890e51630"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80ff03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff80ff03",
    "outputId": "1ff75d0d-6b9e-4be6-cf74-8fc0172580a6"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Print metrics\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mJioiPP7_Cl6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJioiPP7_Cl6",
    "outputId": "49e16c77-d77b-4c51-e776-23db887b4244"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1['processed_text'], df1['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure consistent length for input\n",
    "max_sequence_length = 100  # You can adjust this based on your data\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the number of LSTM units\n",
    "lstm_units = 64\n",
    "\n",
    "# Create a Sequential model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model_lstm.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_lstm, test_accuracy_lstm = model_lstm.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (LSTM): {test_accuracy_lstm * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gQvseweBH_TO",
   "metadata": {
    "id": "gQvseweBH_TO"
   },
   "source": [
    "Let's try using a different deep learning technique, such as a 1D Convolutional Neural Network (1D CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hDIs8AcNHzS_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDIs8AcNHzS_",
    "outputId": "da2656e0-c0a6-47d6-dd63-7377fd2182a9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the number of filters for the Conv1D layer\n",
    "num_filters = 64\n",
    "\n",
    "# Create a Sequential model with 1D CNN\n",
    "model_cnn = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    Conv1D(filters=num_filters, kernel_size=3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_cnn, test_accuracy_cnn = model_cnn.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (1D CNN): {test_accuracy_cnn * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ufO2p6MJaLq",
   "metadata": {
    "id": "3ufO2p6MJaLq"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z1javGsEJfiQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1javGsEJfiQ",
    "outputId": "eac74b7e-fe3a-42ff-ff90-c45a9097e6f7"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_cnn.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_cnn, test_accuracy_cnn = model_cnn.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (1D CNN): {test_accuracy_cnn * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nzSqpWmjGR35",
   "metadata": {
    "id": "nzSqpWmjGR35"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thFvF1WlGR67",
   "metadata": {
    "id": "thFvF1WlGR67"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XIDcXjLLGR9o",
   "metadata": {
    "id": "XIDcXjLLGR9o"
   },
   "outputs": [],
   "source": [
    "# Pad sequences to ensure consistent length for input\n",
    "max_sequence_length = 100  # You can adjust this based on your data\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zRvkQSEvMTif",
   "metadata": {
    "id": "zRvkQSEvMTif"
   },
   "source": [
    "LSTM, used for handling sequential data, excels in managing long-range dependencies.\n",
    "It incorporates memory cells and gates, such as input, output, and forget gates, to regulate the flow of information through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6_h55crE_Cop",
   "metadata": {
    "id": "6_h55crE_Cop"
   },
   "outputs": [],
   "source": [
    "# Define the number of LSTM units\n",
    "lstm_units = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qzG7PlUFGp98",
   "metadata": {
    "id": "qzG7PlUFGp98"
   },
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z-VOgEiVGqA7",
   "metadata": {
    "id": "z-VOgEiVGqA7"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wUyNcH14GqD2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUyNcH14GqD2",
    "outputId": "8d0d0c53-5c82-4107-943a-a4028afd14bc"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model_lstm.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CDaZjWvgGqGN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDaZjWvgGqGN",
    "outputId": "885eac10-6b74-425e-fb92-279f52fa1a3b"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss_lstm, test_accuracy_lstm = model_lstm.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (LSTM): {test_accuracy_lstm * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2xaCSZbJ6Cd",
   "metadata": {
    "id": "G2xaCSZbJ6Cd"
   },
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AdECsqnJMe8q",
   "metadata": {
    "id": "AdECsqnJMe8q"
   },
   "source": [
    "CNNs have found adaptation in sequential tasks. They utilize convolutional layers to capture spatial patterns, making them effective for identifying local features within sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v15LnW3RJ5Kb",
   "metadata": {
    "id": "v15LnW3RJ5Kb"
   },
   "outputs": [],
   "source": [
    "# Create a Sequential model with 1D CNN\n",
    "model_cnn = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    Conv1D(filters=num_filters, kernel_size=3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ga_5-q47J-WF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ga_5-q47J-WF",
    "outputId": "17500e6f-7f97-48e6-e9b0-c720a471c6f4"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_cnn, test_accuracy_cnn = model_cnn.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (1D CNN): {test_accuracy_cnn * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kguAQlLJKkbh",
   "metadata": {
    "id": "kguAQlLJKkbh"
   },
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5jjLoFRbMjtU",
   "metadata": {
    "id": "5jjLoFRbMjtU"
   },
   "source": [
    "GRU is a variant of LSTM designed for sequential data processing with a focus on reducing computational complexity. It employs a simplified gating mechanism to efficiently handle information flow through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXnqmy34Kj0Y",
   "metadata": {
    "id": "rXnqmy34Kj0Y"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "\n",
    "# Define the number of GRU units\n",
    "gru_units = 64\n",
    "\n",
    "# Create a Sequential model with GRU\n",
    "model_gru = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(gru_units, return_sequences=True),\n",
    "    GRU(gru_units),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OpGb4aTvK6Jm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpGb4aTvK6Jm",
    "outputId": "2a10a864-e2ee-4021-f052-9cf4b52a8a4d"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model_gru.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_gru, test_accuracy_gru = model_gru.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (GRU): {test_accuracy_gru * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C-kEBeOhVBeR",
   "metadata": {
    "id": "C-kEBeOhVBeR"
   },
   "source": [
    "**Bidirectional LSTM layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Eek-LCFVBiU",
   "metadata": {
    "id": "5Eek-LCFVBiU"
   },
   "source": [
    "Bidirectional LSTM layers, which process sequences bidirectionally, improving temporal context understanding. The model is compiled with the Adam optimizer and binary crossentropy loss, is suitable for tasks like sentiment analysis or stress detection due to its proficiency in understanding contextual nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_u6FgUKND1F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "i_u6FgUKND1F",
    "outputId": "cc44cfe7-3299-4dd6-c3d0-2563872411d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a Sequential model with Bidirectional LSTM\n",
    "model_bidirectional_lstm = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_bidirectional_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model_bidirectional_lstm.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_bidirectional_lstm, test_accuracy_bidirectional_lstm = model_bidirectional_lstm.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (Bidirectional LSTM): {test_accuracy_bidirectional_lstm * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UQwy-h9QQI0R",
   "metadata": {
    "id": "UQwy-h9QQI0R"
   },
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cSC7rKjEUzjP",
   "metadata": {
    "id": "cSC7rKjEUzjP"
   },
   "source": [
    "  **Embedding Layer:** Converts word index sequences into dense vectors of fixed size.\n",
    "\n",
    "  **GlobalAveragePooling1D():** Performs pooling operation to aggregate information from embeddings.\n",
    "  \n",
    "  **Dense Layers with Dropout:** Dense (fully connected) layers with ReLU activation functions and Dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VWh3UrqsTPA2",
   "metadata": {
    "id": "VWh3UrqsTPA2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a Sequential model with a more complex Neural Network\n",
    "model_nn = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Add dropout for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),  # Add dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "# accuracy as a metric to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdnSLefTUj7O",
   "metadata": {
    "id": "sdnSLefTUj7O"
   },
   "source": [
    "\n",
    "Uses the Adam optimizer and binary crossentropy loss, suitable for binary classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H-RThS0ITgp8",
   "metadata": {
    "id": "H-RThS0ITgp8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model with the Adam optimizer and binary crossentropy loss\n",
    "model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Introduce early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "history_nn = model_nn.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                           validation_data=(X_test_padded, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_nn, test_accuracy_nn = model_nn.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy (Neural Network): {test_accuracy_nn * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nx9KVMSGWH1o",
   "metadata": {
    "id": "nx9KVMSGWH1o"
   },
   "source": [
    "Best score ? Best prediction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-84nAU8Tkcg",
   "metadata": {
    "id": "8-84nAU8Tkcg"
   },
   "outputs": [],
   "source": [
    "# Plot training history (optional)\n",
    "plt.plot(history_nn.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_nn.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uUTR4kplUIHF",
   "metadata": {
    "id": "uUTR4kplUIHF"
   },
   "source": [
    "Y-Axis (Accuracy): Represents the accuracy of the model on the training data.\n",
    "\n",
    "X-Axis (Epoch): Indicates the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I2a-BvHsTm-G",
   "metadata": {
    "id": "I2a-BvHsTm-G"
   },
   "source": [
    "# Test part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KZJbVOQCQ52x",
   "metadata": {
    "id": "KZJbVOQCQ52x"
   },
   "source": [
    "Same example as Part2, lets test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qutCpjCcQy95",
   "metadata": {
    "id": "qutCpjCcQy95"
   },
   "outputs": [],
   "source": [
    "# Les textes à classifier\n",
    "text1 = \"\"\"Work today was tough. Dealing with unexpected challenges and tight deadlines.\n",
    "The pressure is on, and it's affecting my performance.\"\"\"\n",
    "\n",
    "text2 = \"\"\"Had a challenging day at work, but I managed to handle it well.\n",
    "Received positive feedback, boosting my confidence.\"\"\"\n",
    "\n",
    "text3 = \"\"\"The workload is piling up, and the constant pressure from the boss is overwhelming.\n",
    "I'm not sure how much longer I can handle this.\"\"\"\n",
    "\n",
    "text4 = \"\"\"Finished my tasks early today. Enjoying a sense of accomplishment and looking forward to a relaxing evening.\"\"\"\n",
    "\n",
    "text5 = \"\"\"\n",
    "A:Hey, Sarah. The brainstorming session today was intense, huh?\n",
    "\n",
    "B:Yeah, Mark. Intense is one word for it. It feels like the pressure is on to deliver something groundbreaking.\n",
    "\n",
    "A:Exactly. The expectations are high, and we need to step up. There's a lot riding on this project.\n",
    "\n",
    "A:I can't shake off this feeling of unease. What if we miss something important? The stakes seem so high.\n",
    "\n",
    "B:I get it, Sarah. But pressure can sometimes bring out the best in us. Let's channel that stress into motivation.\n",
    "\n",
    "A:I hope so, Mark. I just worry that we're juggling too much. The workload is starting to feel overwhelming.\n",
    "\n",
    "B:It's natural to feel that way. Let's take it one step at a time and lean on each other for support. We got this!\n",
    "\"\"\"\n",
    "\n",
    "# Tokeniser et traiter les textes\n",
    "new_sequences = tokenizer.texts_to_sequences([text1, text2, text3, text4, text5])\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Faire des prédictions pour chaque modèle\n",
    "predictions_lstm = model_lstm.predict(new_padded_sequences)\n",
    "predictions_cnn = model_cnn.predict(new_padded_sequences)\n",
    "predictions_gru = model_gru.predict(new_padded_sequences)\n",
    "predictions_nn = model_nn.predict(new_padded_sequences)\n",
    "\n",
    "# Convertir les prédictions en étiquettes binaires\n",
    "binary_predictions_lstm = [1 if pred > 0.5 else 0 for pred in predictions_lstm]\n",
    "binary_predictions_cnn = [1 if pred > 0.5 else 0 for pred in predictions_cnn]\n",
    "binary_predictions_gru = [1 if pred > 0.5 else 0 for pred in predictions_gru]\n",
    "binary_predictions_nn = [1 if pred > 0.5 else 0 for pred in predictions_nn]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Résultats pour le modèle LSTM : \", binary_predictions_lstm)\n",
    "print(\"Résultats pour le modèle CNN : \", binary_predictions_cnn)\n",
    "print(\"Résultats pour le modèle GRU : \", binary_predictions_gru)\n",
    "print(\"Résultats pour le modèle NN : \", binary_predictions_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I4o56pcZTuXS",
   "metadata": {
    "id": "I4o56pcZTuXS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SS6Sf8nBbpre",
   "metadata": {
    "id": "SS6Sf8nBbpre"
   },
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862Zaup7bqKn",
   "metadata": {
    "id": "862Zaup7bqKn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and pad sequences for BERT\n",
    "X_bert = df1['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128, padding='max_length', truncation=True))\n",
    "X_bert = np.array(list(X_bert))\n",
    "\n",
    "# Split the data\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(X_bert, df1['label'], test_size=0.2, stratify=df1['label'], random_state=0)\n",
    "\n",
    "# BERT Model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the BERT-based model using functional API\n",
    "input_layer = Input(shape=(128,), dtype='int32')\n",
    "bert_output = bert_model(input_layer)[0]\n",
    "pooled_output = GlobalMaxPooling1D()(bert_output)\n",
    "output_layer = Dense(1, activation='sigmoid')(pooled_output)\n",
    "\n",
    "model_bert = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model_bert.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "batch_size = 12\n",
    "model_bert.fit(X_train_bert, y_train_bert, epochs=epochs, batch_size=batch_size, validation_data=(X_test_bert, y_test_bert))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_bert, test_accuracy_bert = model_bert.evaluate(X_test_bert, y_test_bert)\n",
    "print(f'Test Accuracy (BERT): {test_accuracy_bert * 100:.2f}%')\n",
    "\n",
    "# Make predictions for the BERT model\n",
    "predictions_bert = model_bert.predict(X_test_bert)\n",
    "binary_predictions_bert = [1 if pred > 0.5 else 0 for pred in predictions_bert]\n",
    "\n",
    "# Display the results\n",
    "print(\"Résultats pour le modèle BERT : \", binary_predictions_bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IAN9axWjTq5t",
   "metadata": {
    "id": "IAN9axWjTq5t"
   },
   "source": [
    "# RESUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ei8bDWOBLQ",
   "metadata": {
    "id": "59ei8bDWOBLQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy values from the trained models\n",
    "accuracies = [test_accuracy_lstm, test_accuracy_cnn, test_accuracy_gru, test_accuracy_nn, test_accuracy_bidirectional_lstm]\n",
    "models = ['LSTM', 'CNN', 'GRU', 'NN', 'Bidirectional LSTM']\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylim(0, 1)  # Set the y-axis limit to the range of accuracy (0 to 1)\n",
    "\n",
    "# Display the values on top of the bars\n",
    "for i, value in enumerate(accuracies):\n",
    "    plt.text(i, value + 0.01, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2ba03",
   "metadata": {},
   "source": [
    "# Conclusion :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a6f85",
   "metadata": {},
   "source": [
    "Neural Network (NN) emerges as the preeminent model for stress recognition, showcasing its remarkable capabilities in understanding intricate patterns and nuances associated with human emotions. However, it is essential to acknowledge the inherent challenges that persist in accurately identifying stress through machine learning algorithms. \n",
    "\n",
    "Stress, being a highly subjective and complex emotional state, presents difficulties for machines to discern consistently. Integrating additional features, such as personal or medical data, could potentially elevate the precision and efficacy of stress detection models. As we continue to advance in the field of artificial intelligence, refining and expanding these models with diverse datasets and incorporating nuanced features will be crucial for enhancing their performance in recognizing the subtle manifestations of stress in individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd022b6d",
   "metadata": {},
   "source": [
    "Nicolas WATTENHOFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c14d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
